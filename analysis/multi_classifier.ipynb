{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## signals(features)\n",
    "\n",
    "convert lines, words, x, y, w, h in html input tag into singals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_line(contents):\n",
    "    if not contents: \n",
    "        return 0\n",
    "    lines = contents.split('\\n')\n",
    "    return len(lines)\n",
    "\n",
    "def mean_word(contents):\n",
    "    if not contents: \n",
    "        return 0\n",
    "    lines = contents.split('\\n')\n",
    "    words = [len(line.split(' ')) for line in lines]\n",
    "    return np.mean(words)\n",
    "\n",
    "def total_word(contents):\n",
    "    if not contents:\n",
    "        return 0\n",
    "    lines = contents.split('\\n')\n",
    "    words = [len(line.split(' ')) for line in lines]\n",
    "    return sum(words)\n",
    "\n",
    "def std_w(data_w):\n",
    "    if not data_w:\n",
    "        return 0\n",
    "    ws = map(int, data_w.split(','))\n",
    "    return np.std(ws)\n",
    "\n",
    "def mean_y(data_y):\n",
    "    if not data_y:\n",
    "        return 0\n",
    "    ys = map(int, data_y.split(','))\n",
    "    return np.mean([abs(ys[1:][i] - ys[:-1][i]) for i in range(0, len(ys[1:]))])\n",
    "\n",
    "def mean_h(data_h):\n",
    "    if not data_h:\n",
    "        return 0\n",
    "    hs = map(int, data_h.split(','))\n",
    "    return np.mean(hs)\n",
    "\n",
    "def mean_x(data_x):\n",
    "    if not data_x:\n",
    "        return 0\n",
    "    xs = map(int, data_x.split(','))\n",
    "    x_min = np.min(xs)\n",
    "    return np.mean(map(lambda x:x-x_min, xs))\n",
    "\n",
    "def get_page(page):\n",
    "    return int(page.replace('Page', '').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_feature(df, target=1):\n",
    "    df['count_line'] = df['content'].apply(count_line)\n",
    "    df['mean_word'] = df['content'].apply(mean_word)\n",
    "    df['total_word'] = df['content'].apply(total_word)\n",
    "    df['std_w'] = df['data_w'].apply(std_w)\n",
    "    df['mean_y'] = df['data_y'].apply(mean_y)\n",
    "    df['mean_h'] = df['data_h'].apply(mean_h)\n",
    "    df['mean_x'] = df['data_x'].apply(mean_x)\n",
    "    df['page_num'] = df['page'].apply(get_page)\n",
    "    df['target'] = target\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 19)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "poems = pd.read_csv('../data/poem-austlit-20160412.csv')\n",
    "convert_feature(poems, target=1)\n",
    "print poems.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(493, 19)\n"
     ]
    }
   ],
   "source": [
    "# others\n",
    "others = pd.read_csv('../data/others-201604120925.csv')\n",
    "convert_feature(others, target=0)\n",
    "print others.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_df = pd.concat([poems, others])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cross_validation import cross_val_score, KFold, train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import six\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MajorityVoteClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.classifiers = []\n",
    "        self.poem_filename = '../data/poem-austlit-20160412.csv'\n",
    "        self.other_filename = '../data/others-201604120925.csv'\n",
    "\n",
    "    def read_files(self):\n",
    "        poems = pd.read_csv(self.poem_filename)\n",
    "        convert_feature(poems, target=1)\n",
    "        \n",
    "        others = pd.read_csv(self.other_filename)\n",
    "        convert_feature(others, target=0)\n",
    "        \n",
    "        self.all_df = pd.concat([poems, others])\n",
    "        \n",
    "    def process_data(self):\n",
    "        format_features = ['count_line', 'mean_word', 'total_word', 'std_w', 'mean_y', 'mean_h', 'mean_x', 'page_num']\n",
    "        content_features = 'content'\n",
    "        y_feature = ['target']\n",
    "        \n",
    "        X_format = self.all_df[format_features].values\n",
    "        X_format[X_format > 1000] = 1000\n",
    "        X_format[np.isnan(X_format)] = 0\n",
    "        \n",
    "        X_content = self.all_df[content_features].values\n",
    "        X_content = CountVectorizer(max_df=10, min_df=1).fit_transform(X_content)\n",
    "        X_content = TfidfTransformer().fit_transform(X_content)\n",
    "        \n",
    "        y = self.all_df[y_feature].values\n",
    "        y = y.flatten()\n",
    "        \n",
    "        self.X_format_train, self.X_format_test, self.y_format_train, self.y_format_test = train_test_split(X_format, y, test_size=0.20, random_state=42)\n",
    "        self.X_content_train, self.X_content_test, self.y_content_train, self.y_content_test = train_test_split(X_content, y, test_size=0.20, random_state=42)\n",
    "        \n",
    "    def predict(self):\n",
    "        count = len(self.y_content_test)\n",
    "        predictions = [self._predict(self.X_format_test[i, :], self.X_content_test[i, :]) for i in range(0, count)]\n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        print(sum(predictions==self.y_content_test)/count)\n",
    "        return sum(predictions==self.y_content_test)/count\n",
    "    \n",
    "    def _predict(self, X_format, X_content):\n",
    "        return self.predict_proba(X_format, X_content)\n",
    "    \n",
    "    def predict_proba(self, X_format, X_content):\n",
    "        probas = []\n",
    "        predicts = []\n",
    "        for name, clf in self.classifiers:\n",
    "            if name == 'RandomForest':\n",
    "                X = X_format\n",
    "            else:\n",
    "                X = X_content\n",
    "            probas.append(clf.predict_proba(X))\n",
    "            predicts.append(clf.predict(X))\n",
    "            \n",
    "        probas = np.asarray(probas)\n",
    "        n = np.argmax(probas)\n",
    "        if n in [0, 1]:\n",
    "            return predicts[0][0]\n",
    "        elif n in [2, 3]:\n",
    "            return predicts[1][0]\n",
    "        else:\n",
    "            return predicts[2][0]\n",
    "    \n",
    "    def run(self):\n",
    "        print('read **************************************')\n",
    "        self.read_files()\n",
    "        print('process_data ******************************')\n",
    "        self.process_data()\n",
    "        print('train_classifier **************************')\n",
    "        self.triain_rfclassifier()\n",
    "        self.triain_nbclassifier()\n",
    "        self.triain_lrclassifier()\n",
    "        print('predict ***********************************')\n",
    "        print self.predict()\n",
    "        \n",
    "    def triain_rfclassifier(self):\n",
    "        X, y = self.X_format_train, self.y_format_train\n",
    "        clf = Pipeline([\n",
    "                    ('clf', RandomForestClassifier()),\n",
    "                ])\n",
    "\n",
    "        parameters = {\n",
    "                        'clf__n_estimators': (5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70), \n",
    "                        'clf__criterion': ('gini', 'entropy'),\n",
    "                        'clf__max_features': ('auto', 'sqrt', 'log2')\n",
    "                     }    \n",
    "\n",
    "        gs_clf = GridSearchCV(clf, parameters, cv=5)\n",
    "        gs_clf.fit(X, y)\n",
    "\n",
    "        best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
    "        print('RandomForestClassifier')\n",
    "        print('Score : ', score)\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "        self.classifiers.append(('RandomForest', gs_clf.best_estimator_))\n",
    "    \n",
    "    def triain_nbclassifier(self):\n",
    "        X, y = self.X_content_train, self.y_content_train\n",
    "        \n",
    "        clf = Pipeline([\n",
    "                        ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "        parameters = { 'clf__alpha': (0,1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4), \n",
    "                        'clf__fit_prior':(True, False)}    \n",
    "\n",
    "        gs_clf = GridSearchCV(clf, parameters, cv=5)\n",
    "        gs_clf.fit(X, y)\n",
    "\n",
    "        best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
    "        print('MultinomialNB')\n",
    "        print('Score : ', score)\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "        self.classifiers.append(('MultinomialNB', gs_clf.best_estimator_))\n",
    "    \n",
    "    def triain_lrclassifier(self):\n",
    "        X, y = self.X_content_train, self.y_content_train\n",
    "        \n",
    "        clf = Pipeline([\n",
    "                        ('clf', LogisticRegression()),\n",
    "                    ])\n",
    "\n",
    "        parameters = { \n",
    "#                         'clf__alpha': (0,1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4), \n",
    "#                         'clf__fit_prior':(True, False)\n",
    "                     }    \n",
    "\n",
    "        gs_clf = GridSearchCV(clf, parameters, cv=5)\n",
    "        gs_clf.fit(X, y)\n",
    "\n",
    "        best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
    "        print('LogisticRegressionClassifier')\n",
    "        print('Score : ', score)\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "        self.classifiers.append(('LogisticRegression', gs_clf.best_estimator_))\n",
    "    \n",
    "    def save(self):\n",
    "        joblib.dump(clf, 'filename.pkl') \n",
    "        joblib.dump(clf, 'filename.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read **************************************\n",
      "process_data ******************************\n",
      "train_classifier **************************\n",
      "RandomForestClassifier\n",
      "('Score : ', 0.91561712846347609)\n",
      "clf__criterion: 'entropy'\n",
      "clf__max_features: 'auto'\n",
      "clf__n_estimators: 60\n",
      "MultinomialNB\n",
      "('Score : ', 0.88539042821158687)\n",
      "clf__alpha: 0.3\n",
      "clf__fit_prior: False\n",
      "LogisticRegressionClassifier\n",
      "('Score : ', 0.92695214105793455)\n",
      "predict ***********************************\n",
      "0.949748743719\n",
      "0.949748743719\n"
     ]
    }
   ],
   "source": [
    "mvc = MajorityVoteClassifier()\n",
    "mvc.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
